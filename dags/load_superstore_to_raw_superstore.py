from datetime import datetime
import pandas as pd
import psycopg2
from psycopg2.extras import execute_values

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator 

CSV_PATH = "/opt/airflow/data/raw/csv/superstore.csv"

# ðŸŸ¢ Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐž: ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ÑÑ Ðº Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ð¹ Ð±Ð°Ð·Ðµ dwh_raw
PG_HOST = "postgres"
PG_DB = "dwh_raw"                    # â† Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐž Ð¡ airflow ÐÐ dwh_raw
PG_USER = "airflow"
PG_PASSWORD = "airflow"
TABLE_NAME = "raw.superstore_raw"

def load_superstore_to_raw():
    #Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼ csv
    df = pd.read_csv(CSV_PATH, encoding="cp1251")

    if df.empty:
        print("Ñ„Ð°Ð¹Ð» superstore.csv Ð¿ÑƒÑÑ‚Ð¾Ð¹, Ð½ÐµÑ‡ÐµÐ³Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°Ñ‚ÑŒ")
        return
    columns = list(df.columns)

     # 2. Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ÑÑ Ðº Postgres
    conn = psycopg2.connect(
        host=PG_HOST,
        dbname=PG_DB,
        user=PG_USER,
        password=PG_PASSWORD,
    )
    conn.autocommit = True
    cur = conn.cursor()

    # ðŸŸ¢ Ð”ÐžÐ‘ÐÐ’Ð›Ð•ÐÐž: Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑÑ…ÐµÐ¼Ñƒ raw ÐµÑÐ»Ð¸ Ð½Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚
    cur.execute("CREATE SCHEMA IF NOT EXISTS raw;")
    
    # 3. ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ, ÐµÑÐ»Ð¸ ÐµÐµ Ð½ÐµÑ‚ (Ð²ÑÐµ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ ÐºÐ°Ðº text Ð´Ð»Ñ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ñ‚Ñ‹)
    columns_ddl = ",\n    ".join([f'"{col}" text' for col in columns])
    create_table_sql = f'''
    CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
        {columns_ddl}
    );
    '''
    cur.execute(create_table_sql)

    # ðŸŸ¢ Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐž: Ð˜Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð²Ð¼ÐµÑÑ‚Ð¾ TRUNCATE
    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð´Ð°Ñ‚Ñƒ Ð¸Ð· ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
    cur.execute(f'SELECT MAX("Order_Date") FROM {TABLE_NAME}')
    last_date_result = cur.fetchone()[0]
    
    # ðŸ”´ðŸ”´ðŸ”´ Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐž: Ð’Ð•Ð ÐÐ£Ð¢Ð¬ ÐŸÐ ÐÐ’Ð˜Ð›Ð¬ÐÐ«Ð• ÐžÐ¢Ð¡Ð¢Ð£ÐŸÐ« ðŸ”´ðŸ”´ðŸ”´
    if last_date_result:
        # ðŸŸ¢ Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐž: ÐŸÑ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ð´Ð°Ñ‚Ñ‹ DD/MM/YYYY
        df['Order_Date'] = pd.to_datetime(df['Order_Date'], format='%d/%m/%Y')
        last_date = pd.to_datetime(last_date_result)
        
        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð¾Ð²Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
        new_data = df[df['Order_Date'] > last_date]
        print(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(new_data)} Ð½Ð¾Ð²Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð¿Ð¾ÑÐ»Ðµ {last_date}")
    else:
        new_data = df  # ÐŸÐµÑ€Ð²Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ°
        print("ÐŸÐµÑ€Ð²Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð²ÑÐµÑ… Ð´Ð°Ð½Ð½Ñ‹Ñ…")

    if not new_data.empty:
        # 4. Ð³Ð¾Ñ‚Ð¾Ð²Ð¸Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ðº Ð²ÑÑ‚Ð°Ð²ÐºÐµ
        rows = list(new_data.itertuples(index=False, name=None))

        insert_columns = ", ".join([f'"{col}"' for col in columns])
        insert_sql = f"INSERT INTO {TABLE_NAME} ({insert_columns}) VALUES %s"

        # 5. Ð¼Ð°ÑÑÐ¾Ð²Ð°Ñ Ð²ÑÑ‚Ð°Ð²ÐºÐ° Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
        execute_values(cur, insert_sql, rows)
        print(f"Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¾ Ð½Ð¾Ð²Ñ‹Ñ… ÑÑ‚Ñ€Ð¾Ðº: {len(rows)}")
    else:
        print("ÐÐµÑ‚ Ð½Ð¾Ð²Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸")

    cur.close()
    conn.close()
    # ðŸ”´ðŸ”´ðŸ”´ ÐšÐžÐÐ•Ð¦ Ð˜Ð¡ÐŸÐ ÐÐ’Ð›Ð•ÐÐ˜Ð¯ ÐžÐ¢Ð¡Ð¢Ð£ÐŸÐžÐ’ ðŸ”´ðŸ”´ðŸ”´

with DAG(
    dag_id="superstore_etl",
    start_date=datetime(2025, 1, 1),
    schedule_interval=None,
    catchup=False,
    tags=["superstore", "etl", "bronze-silver-gold"],
) as dag:

    # Ð­Ñ‚Ð°Ð¿ 1: Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð² Bronze (Ð¸Ð½ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ð°Ñ)
    load_to_bronze = PythonOperator(
        task_id="load_to_bronze",
        python_callable=load_superstore_to_raw,
    )

    # Ð­Ñ‚Ð°Ð¿ 2: Ð—Ð°Ð¿ÑƒÑÐº dbt Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¹
    dbt_run = BashOperator(
        task_id="run_dbt_transformations",
        bash_command='cd /opt/airflow/dbt && dbt run',
    )

    # Ð­Ñ‚Ð°Ð¿ 3: Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…
    dbt_test = BashOperator(
        task_id="test_data_quality",
        bash_command='cd /opt/airflow/dbt && dbt test',
    )

    # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ
    load_to_bronze >> dbt_run >> dbt_test